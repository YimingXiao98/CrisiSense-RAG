"""Generation evaluation metrics using LLM-as-a-Judge."""

from __future__ import annotations

import json
from typing import Any, Dict, List

from loguru import logger

from ..models.vlm_client import VLMClient


class GenerationEvaluator:
    """Evaluates generated RAG answers using an LLM judge."""

    def __init__(self, client: VLMClient) -> None:
        self.client = client

    def evaluate(
        self,
        query: str,
        context: Dict[str, Any],
        answer: str,
        query_params: Dict[str, str] = None,
    ) -> Dict[str, float]:
        """Compute generation metrics: Faithfulness and Relevance."""

        # 1. Faithfulness (Hallucination Check)
        faithfulness_score = self._evaluate_faithfulness(context, answer, query_params)

        # 2. Relevance (Helpfulness Check)
        relevance_score = self._evaluate_relevance(query, answer)

        return {"faithfulness": faithfulness_score, "relevance": relevance_score}

    def _evaluate_faithfulness(
        self, context: Dict[str, Any], answer: str, query_params: Dict[str, str] = None
    ) -> float:
        """Check if the answer is supported by the context."""
        # Prepare context summary
        snippets = "\n".join(context.get("text_snippets", []))
        kb_summary = context.get("kb_summary", "")

        # Add imagery metadata to context so judge knows what images exist
        image_ids = [
            t.get("tile_id", "unknown") for t in context.get("imagery_tiles", [])
        ]
        imagery_context = f"Available Imagery Tiles: {', '.join(image_ids)}"

        # Add query context
        query_context = ""
        if query_params:
            query_context = f"Query Parameters: ZIP={query_params.get('zip')}, Start={query_params.get('start')}, End={query_params.get('end')}"

        prompt = f"""
        You are a strict fact-checking judge. 
        Your task is to verify if the following ANSWER is supported by the provided CONTEXT.
        
        CONTEXT:
        {query_context}
        {imagery_context}
        {snippets}
        {kb_summary}
        
        ANSWER:
        {answer}
        
        INSTRUCTIONS:
        1. Read the Answer carefully and identify all factual claims.
        2. For each claim, check if it is supported by the Context.
        3. IMPORTANT: The Answer was generated by a VLM that CAN see the images listed in 'Available Imagery Tiles'. If the Answer describes visual details of these images (e.g. flooding, debris), assume those details are SUPPORTED by the existence of the image ID, unless directly contradicted by other text context.
        4. ALLOW reasonable inferences based on general knowledge (e.g., "floodwaters recede slowly", "heavy rain causes flooding").
        5. Calculate a faithfulness score from 0.0 to 1.0:
           - 1.0 = All claims are fully supported by the context
           - 0.7-0.9 = Most claims supported, minor unsupported details
           - 0.4-0.6 = Mix of supported and unsupported claims
           - 0.1-0.3 = Few claims supported, mostly unsupported
           - 0.0 = Completely unsupported or contradicts context
        
        Respond in the following JSON format:
        {{
            "reasoning": "Explain which claims are supported/unsupported and why...",
            "supported_claims": ["list of supported claims"],
            "unsupported_claims": ["list of unsupported claims"],
            "score": <float between 0.0 and 1.0>
        }}
        """

        try:
            # We use the text-only mode of the VLM client for this
            response = self.client._generate_text(prompt)
            logger.info(f"Faithfulness check | Response: {response}")

            # clean up markdown code blocks if present
            cleaned = response.replace("```json", "").replace("```", "").strip()
            data = json.loads(cleaned)
            return float(data["score"])
        except Exception as e:
            logger.warning(f"Faithfulness check failed: {e}")
            return 0.0

    def _evaluate_relevance(self, query: str, answer: str) -> float:
        """Check if the answer addresses the user's query."""
        prompt = f"""
        You are a helpful assistant evaluator.
        Your task is to rate how well the ANSWER addresses the USER QUERY.
        
        USER QUERY:
        {query}
        
        ANSWER:
        {answer}
        
        INSTRUCTIONS:
        Rate the relevance on a scale of 0.0 to 1.0:
        - 1.0: Perfectly answers the query with specific details.
        - 0.5: Partially answers the query but misses key aspects.
        - 0.0: Irrelevant or fails to answer the query.
        
        Respond with ONLY the score (e.g., 0.8).
        """

        try:
            response = self.client._generate_text(prompt)
            score = float(response.strip())
            return score
        except Exception as e:
            logger.warning(f"Relevance check failed: {e}")
            return 0.0
