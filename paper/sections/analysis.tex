% Analysis Section
\section{Analysis}
\label{sec:analysis}

\subsection{Ablation Study}
\label{sec:ablation_analysis}

We conduct ablation experiments to understand the contribution of each component.

\input{tables/ablation}

\paragraph{Modality Ablation.}
Our dual-metric evaluation (Table~\ref{tab:main_results}) reveals that proper handling of image captions significantly improves performance over text-only approaches.

\textbf{Caption Success with Temporal Context}: Our Text+Caption pipeline achieves the best performance on both metrics: \textbf{20.16\%} MAE for Flood Extent (a 2.54 percentage point improvement over Text-Only) and \textbf{26.39\%} MAE for Damage Severity (a 2.86 percentage point improvement). This success stems from explicit temporal context instructions that teach the LLM to correctly interpret satellite imagery captions:
\begin{itemize}
    \item Captions describe imagery captured on August 31st, \textit{after} peak flooding (August 27--28).
    \item ``No visible flooding'' means water \textit{receded}, not that flooding didn't occur---the LLM is instructed to trust text for flood extent.
    \item Damage indicators (debris, structural damage) in captions \textit{confirm} text reports, boosting confidence.
\end{itemize}

\textbf{Importance of Prompt Engineering}: Without proper temporal context, naive caption fusion \textit{degraded} performance in prior experiments (MAE increased by 2+ points). The key insight is that captions are not inherently noisy---they require interpretation guidance to resolve the apparent contradiction between post-flood imagery and real-time text reports.

\paragraph{Architecture Ablation.}
We also evaluate architectural choices. Direct prompting without RAG significantly degrades performance, underscoring the importance of retrieval-augmented context. Removing the cross-encoder reranker reduces response quality, indicating that refined candidate selection contributes meaningfully to accuracy. Finally, we find that simply averaging text and visual estimates underperforms our learned fusion approach, suggesting that modality integration benefits from trainable parameters.

\subsection{Geographic Boosting Analysis}
\label{sec:geo_boosting}

We investigated whether prioritizing documents strictly from the query's ZIP code would improve accuracy (Exp2). Our hypothesis was that local context is most predictive. However, results were \textbf{negative}: MAE increased from 3.07\% (Baseline) to 4.94\% (Geo-Enhanced).

Analysis revealed that boosting local documents often surfaced irrelevant reports (e.g., ``blocked driveway'') over highly relevant but distant descriptions (e.g., ``major flooding nearby''). This finding suggests that \textbf{semantic relevance is more critical than strict geographic proximity} for disaster impact assessment.

\subsection{Error Analysis}
\label{sec:error_analysis}

%Figure~\ref{fig:error_cases} shows representative error cases. %TODO: add figure reference

\begin{figure}[t]
    \centering
    % Placeholder for error case examples
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}
            \textbf{[PLACEHOLDER: Error Case Examples]}\\
            Show: 2-3 failure cases with analysis
            \vspace{2cm}}}
    \caption{Representative error cases. Top: \todo{description}. Bottom: \todo{description}.}
    \label{fig:error_cases}
\end{figure}

\paragraph{Common Error Types.}
Our analysis reveals three primary categories of errors. First, context gaps occur in ZIP codes with sparse data coverage, where insufficient evidence leads to unreliable predictions. Second, temporal misalignment arises when the system confuses pre-storm and post-storm imagery, resulting in incorrect damage assessments. Third, false positives stem from misinterpreting normal water features such as swimming pools or retention ponds as flood damage.

\subsection{Retrieval Quality}
\label{sec:retrieval_analysis}

We analyze retrieval effectiveness using standard IR metrics.

\input{tables/retrieval_metrics}

\subsection{Qualitative Examples}
\label{sec:qualitative}

%Figure~\ref{fig:qualitative} shows example system outputs. %TODO: add figure reference

\begin{figure}[t]
    \centering
    % Placeholder for qualitative examples
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{3cm}
            \textbf{[PLACEHOLDER: Qualitative Examples]}\\
            Show: Query + Retrieved Context + Model Response + Ground Truth\\
            For 2-3 representative cases
            \vspace{3cm}}}
    \caption{Example system outputs showing retrieved context and generated assessments.}
    \label{fig:qualitative}
\end{figure}
