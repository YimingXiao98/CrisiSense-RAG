% Results Section
\section{Results}
\label{sec:results}

\subsection{Main Results}
\label{sec:main_results}

Table~\ref{tab:main_results} presents the main quantitative results from our ablation study comparing three configurations: text-only, text with captions, and full multimodal RAG.

\input{tables/main_results}


\paragraph{Key Findings.}
Our results demonstrate that image captions, when properly interpreted, significantly improve disaster assessment:

\textbf{(1) Text+Caption Achieves Best Performance.} With proper temporal context instructions, the Text+Caption pipeline achieves the lowest error on both metrics: \textbf{20.16\%} MAE for Flood Extent and \textbf{26.39\%} MAE for Damage Severity. This represents a 2.54 and 2.86 percentage point improvement over Text-Only, respectively.

\textbf{(2) Temporal Context is Critical.} The key to successful caption fusion is teaching the LLM that satellite imagery was captured \textit{after} peak flooding. Without this context, captions like ``no visible flooding'' contradict text evidence and degrade performance. With proper instructions, the LLM correctly interprets these as ``water receded'' rather than ``flooding didn't occur.''

\textbf{(3) Captions Provide Complementary Signals.} Image captions offer damage indicators (debris, structural damage, discoloration) that confirm and contextualize text reports, while text sources provide real-time flood extent information that imagery cannot capture due to temporal mismatch.

\subsection{Per-Category Performance}
\label{sec:category_results}

We analyze performance across different flood severity levels to understand where multimodal context provides the greatest benefit.

\begin{figure}[t]
    \centering
    % Placeholder for category performance bar chart
    \fbox{\parbox{0.9\textwidth}{\centering\vspace{2cm}
            \textbf{[PLACEHOLDER: Bar Chart]}\\
            Show: MAE by flood depth category (High/Medium/Low) for each model
            \vspace{2cm}}}
    \caption{Performance by flood severity. \systemname{} shows consistent improvements across all damage levels, with the largest gains in high-severity regions where visual confirmation is most critical.}
    \label{fig:category_performance}
\end{figure}

\input{tables/category_results}

\subsection{Ablation Analysis}
\label{sec:ablation}

\paragraph{Impact of Temporal Context.}
The success of our Text+Caption approach hinges on explicit temporal context instructions. Without guidance, the LLM sees conflicting evidence: tweets report ``house flooded'' while captions state ``no visible flooding.'' By explaining that imagery was captured days after peak flooding, the LLM correctly resolves this conflict---trusting text for flood extent while using caption damage indicators as confirmatory evidence.

\paragraph{Role of Image Captions.}
With proper interpretation guidance, image captions provide valuable complementary signals. Captions excel at describing persistent damage (debris fields, structural collapse, vegetation damage) that remains visible in post-flood imagery. This information confirms and contextualizes text reports, reducing overall error by 2.5--2.9 percentage points compared to text-only approaches.

% NOTE: LLM-as-a-Judge and Human-AI Agreement sections are commented out as these
% evaluations are not included in the current experimental scope. They are implemented
% in the codebase and can be enabled for future work.
%
% \subsection{LLM Judge Evaluation}
% \subsection{Human-AI Agreement}
