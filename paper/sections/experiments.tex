% Experiments Section
\section{Experiments}
\label{sec:experiments}

\subsection{Dataset}
\label{sec:dataset}

We evaluate \systemname{} on data from Hurricane Harvey, which made landfall on August 25, 2017, causing unprecedented flooding in the Greater Houston area. %Table~\ref{tab:dataset_stats} summarizes our dataset. %TODO: add table reference

\input{tables/dataset_stats}


\paragraph{Dual Ground Truth.}
We employ a dual ground-truth framework to evaluate the distinct "Hazard" and "Consequence" outputs of our system:

\begin{itemize}
    \item \textbf{Hazard Ground Truth (Flood Extent)}: We use the FEMA Harvey Flood Depth Grid to compute the percentage of each ZIP code covered by water (\texttt{flooded\_pct}). This serves as the target for our model's \texttt{flood\_extent\_pct} prediction.
    \item \textbf{Damage Ground Truth (Structural Severity)}: We utilize Point Damage Estimates (PDE) and FEMA NFIP Claims to quantify structural impact. The PDE score (normalized 0--100) serves as the primary target for our model's \texttt{damage\_severity\_pct} prediction, representing the intensity of physical destruction rather than just water presence.
\end{itemize}

\paragraph{Query Construction.}
We construct 50 evaluation queries with a stratified sampling design to ensure diverse geographic and data coverage. The evaluation set comprises 35 queries from ZIP codes with high 311 call volume (representing urban Houston with dense ground-truth signals) and 15 queries from suburban/peripheral ZIP codes. All queries target the Hurricane Harvey impact period (August 25--September 10, 2017) with 7-day rolling windows.

\subsection{Ablation Study Design}
\label{sec:baselines}

We conduct a systematic ablation study to quantify the contribution of each modality:

\paragraph{Text-Only RAG.}
Baseline RAG using only text sources (tweets, 311 calls, sensors) without any imagery input. This represents a purely text-based assessment approach.

\paragraph{Text + Caption RAG.}
RAG with text sources augmented by image captions. Captions are generated from satellite imagery using Gemini 2.0 Flash and indexed as searchable text. This tests whether semantic descriptions of imagery can bridge the modality gap without direct visual analysis.

\paragraph{Full Multimodal RAG (\systemname{}).}
Complete split-pipeline architecture with separate Text Analyst and Visual Analyst modules. The Visual Analyst directly processes satellite imagery tiles using a vision-language model, providing independent visual assessment that is fused with text analysis.

\paragraph{Temporal Context for Caption Fusion.}
A critical challenge in early fusion via captioning is the temporal mismatch between text sources (reporting real-time conditions during peak flooding) and satellite imagery captions (describing post-flood scenes). We address this through explicit prompt engineering that provides temporal context:
\begin{itemize}
    \item \textbf{For Flood Extent (Hazard)}: The LLM is instructed to \textit{ignore} caption statements about flooding (e.g., ``no visible flooding'') and trust text sources, since water recedes before image capture.
    \item \textbf{For Damage Severity (Consequence)}: The LLM is instructed to \textit{use} caption damage indicators (debris, structural damage, discoloration) as confirmatory evidence that boosts confidence in text-reported damage.
\end{itemize}
This interpretation guidance resolves the apparent contradiction between modalities, enabling captions to provide complementary value rather than introducing noise.

\subsection{Models}
\label{sec:models}

We evaluate multiple model configurations:

\input{tables/models}

\subsection{Evaluation Metrics}
\label{sec:metrics}

\paragraph{Quantitative Metrics.}
We evaluate prediction accuracy using two complementary metrics: Mean Absolute Error (MAE) measures the average deviation between predicted flood impact and actual flood depth in meters; Spearman's $\rho$ assesses rank correlation with ground truth, indicating whether the system correctly orders regions by flood severity.

% NOTE: Quality metrics (Faithfulness, Relevance) and Human Evaluation are implemented
% in the codebase but not included in the current experimental scope.
% 
% \paragraph{Quality Metrics.}
% Beyond quantitative accuracy, we assess response quality through Faithfulness 
% (proportion of claims supported by retrieved context) and Relevance (query-response alignment).
% 
% \subsection{Human Evaluation}
% To validate LLM judge scores, we conduct human annotation on selected queries.
